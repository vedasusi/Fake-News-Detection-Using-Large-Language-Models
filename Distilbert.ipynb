{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e204080e-de55-42e8-965d-c4f03579cea8",
   "metadata": {},
   "source": [
    "DistilBERT, a smaller and faster version of BERT, was used in our fake news detection project with the FakeNewsNet dataset for:\n",
    "\n",
    "Text Tokenization:\n",
    "\n",
    "Used DistilBertTokenizerFast to convert raw news text into tokenized inputs (subword tokens, attention masks) for processing.\n",
    "Feature Extraction:\n",
    "\n",
    "Generated dense vector embeddings from news text, capturing contextual meaning efficiently.\n",
    "Classification Task:\n",
    "\n",
    "Fine-tuned DistilBERT on the FakeNewsNet dataset to classify news articles as fake or real based on text features.\n",
    "Efficiency & Speed:\n",
    "\n",
    "DistilBERT retained 97% of BERT's performance but was 60% faster and required 40% fewer parameters, making it well-suited for your model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e926c3-1d3b-4584-9926-249c321960ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved as 'text_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load each CSV file\n",
    "gossipcop_real = pd.read_csv(\"gossipcop_real.csv\")\n",
    "gossipcop_fake = pd.read_csv(\"gossipcop_fake.csv\")\n",
    "politifact_real = pd.read_csv(\"politifact_real.csv\")\n",
    "politifact_fake = pd.read_csv(\"politifact_fake.csv\")\n",
    "\n",
    "# Add a new 'label' column to each dataset\n",
    "gossipcop_real[\"label\"] = \"real\"\n",
    "politifact_real[\"label\"] = \"real\"\n",
    "gossipcop_fake[\"label\"] = \"fake\"\n",
    "politifact_fake[\"label\"] = \"fake\"\n",
    "\n",
    "# Combine all datasets into one DataFrame\n",
    "combined_dataset = pd.concat(\n",
    "    [gossipcop_real, gossipcop_fake, politifact_real, politifact_fake],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Optionally, shuffle the combined dataset for better mixing\n",
    "combined_dataset = combined_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the merged dataset to a CSV file\n",
    "combined_dataset.to_csv(\"text_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Merged dataset saved as 'text_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "863c4b74-4193-4526-be26-5882b6758661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vedas\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the merged dataset\n",
    "df = pd.read_csv(\"text_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f06db63-ef2c-4233-8274-8a17729861b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels: 'real' -> 0, 'fake' -> 1\n",
    "df['label'] = df['label'].map({'real': 0, 'fake': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b25926ec-19d6-40c8-b6a5-bd63303dfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'title' column as the text input\n",
    "df = df[['title', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d641ff03-6a95-444e-867f-3d2ddb914331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Hugging Face Dataset from the DataFrame\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b807d4d2-d463-45f1-bda2-d25bb58dbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80/20 split)\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a3ed34a-332b-4186-aa52-19da0a173f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb24481b4494d33a020c0200973a40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vedas\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vedas\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282b015e684c47d2a647a5850abf096b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d70fe665248490887bd62a157f94dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de09f53377614b6c8921dacedd6a5e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer for DistilBERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e1fdd6f-0199-4d87-864c-b8df97eb06e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c976874ee9844a4bda1a13484641b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d8d21b9da1424a87974c3149ce090e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset using the 'title' field\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"title\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = split_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9e07ca8-2f87-45db-909c-973860d6e773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b73bb5ac0e2450ab01aa4edcb85b738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the DistilBERT model for sequence classification (2 labels: real and fake)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93aeb2d0-856c-4f24-8086-2d755718bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2221264-6d0b-4528-bd78-d15450864350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ab7fc8d-7986-4283-b73b-87767047b178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3480' max='3480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3480/3480 5:54:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>0.334748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.343741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.473632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3480, training_loss=0.27514485072815553, metrics={'train_runtime': 21273.5729, 'train_samples_per_second': 2.617, 'train_steps_per_second': 0.164, 'total_flos': 1843548787095552.0, 'train_loss': 0.27514485072815553, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b238d696-12b2-4ce3-bfe9-5dfb1d4d0f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. The model and tokenizer are saved in './text_classifier_model'\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer for later use\n",
    "model.save_pretrained(\"./text_classifier_model\")\n",
    "tokenizer.save_pretrained(\"./text_classifier_model\")\n",
    "\n",
    "print(\"Training complete. The model and tokenizer are saved in './text_classifier_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4848270-ef01-4d9a-bec4-42556df96a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 10:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.473632276058197, 'eval_accuracy': 0.8668103448275862, 'eval_precision': 0.8635080568086245, 'eval_recall': 0.8668103448275862, 'eval_f1': 0.864629131973045, 'eval_runtime': 624.6948, 'eval_samples_per_second': 7.428, 'eval_steps_per_second': 0.464, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Define a function to compute evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    \n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Add the evaluation function to Trainer\n",
    "trainer.compute_metrics = compute_metrics\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "879f94ab-e580-4edb-9ff6-08166c8e7517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        real       0.95      0.97      0.96       378\n",
      "        fake       0.91      0.85      0.88       122\n",
      "\n",
      "    accuracy                           0.94       500\n",
      "   macro avg       0.93      0.91      0.92       500\n",
      "weighted avg       0.94      0.94      0.94       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = \"./text_classifier_model\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# Load the test dataset\n",
    "import pandas as pd\n",
    "df_test = pd.read_csv(\"text_dataset.csv\").sample(500)  # Sample 500 test cases for evaluation\n",
    "df_test[\"label\"] = df_test[\"label\"].map({\"real\": 0, \"fake\": 1})  # Convert labels to numerical format\n",
    "\n",
    "# Predict on test data\n",
    "y_true = df_test[\"label\"].tolist()\n",
    "y_pred = []\n",
    "\n",
    "for text in df_test[\"title\"]:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    prediction = torch.argmax(probabilities, dim=1).item()\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred, target_names=[\"real\", \"fake\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d270ebe-b6e4-4981-adbd-8131ef585301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\vedas\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\vedas\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vedas\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vedas\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a33e163-b259-4686-9521-f320679d5161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        real       0.95      0.97      0.96       378\n",
      "        fake       0.91      0.85      0.88       122\n",
      "\n",
      "    accuracy                           0.94       500\n",
      "   macro avg       0.93      0.91      0.92       500\n",
      "weighted avg       0.94      0.94      0.94       500\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = \"./text_classifier_model\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# Load the test dataset (using 500 samples for visualization)\n",
    "df_test = pd.read_csv(\"text_dataset.csv\").sample(500, random_state=42)\n",
    "df_test[\"label\"] = df_test[\"label\"].map({\"real\": 0, \"fake\": 1})  # Convert labels to numeric\n",
    "\n",
    "# Predict on test data\n",
    "y_true = df_test[\"label\"].tolist()\n",
    "y_pred = []\n",
    "\n",
    "for text in df_test[\"title\"]:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    prediction = torch.argmax(probabilities, dim=1).item()\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "# Generate Classification Report\n",
    "print(classification_report(y_true, y_pred, target_names=[\"real\", \"fake\"]))\n",
    "\n",
    "# Generate Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Heatmap\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"real\", \"fake\"], yticklabels=[\"real\", \"fake\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69be58-93cc-4a1c-bd63-0884c387b783",
   "metadata": {},
   "source": [
    "**TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06c8beeb-ec72-4eb4-b15b-d83bdd07993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Real (Confidence: 0.92)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = \"./text_classifier_model\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Function to classify a news headline\n",
    "def classify_news(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    prediction = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    label = \"Real\" if prediction == 0 else \"Fake\"\n",
    "    confidence = probabilities[0][prediction].item()\n",
    "    \n",
    "    return label, confidence\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    test_text = \"Government announces new policies to tackle inflation.\"\n",
    "    label, confidence = classify_news(test_text)\n",
    "    \n",
    "    print(f\"Prediction: {label} (Confidence: {confidence:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3b8e52d-56d0-420a-a2bf-157f39509a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Real (Confidence: 0.97)\n"
     ]
    }
   ],
   "source": [
    "test_text = \"For The Love Of God, Why Can't Anyone Write Kate McKinnon A Good Movie Role?\"\n",
    "label, confidence = classify_news(test_text)\n",
    "    \n",
    "print(f\"Prediction: {label} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a79a870f-a971-4ae7-8828-4c1d222754a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Fake (Confidence: 0.99)\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Breaking: Scientists confirm that drinking coffee cures cancer overnight! Major health organizations are hiding this fact to protect pharmaceutical profits.\"\n",
    "label, confidence = classify_news(test_text)\n",
    "    \n",
    "print(f\"Prediction: {label} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2285cb75-b6ad-4a4a-8b3d-90249e97e759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Real (Confidence: 0.93)\n"
     ]
    }
   ],
   "source": [
    "test_text = \"NASA successfully lands rover on Mars, begins exploration mission.\"\n",
    "label, confidence = classify_news(test_text)\n",
    "    \n",
    "print(f\"Prediction: {label} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3283a90b-2af2-486a-9479-a7ccf17ceb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Fake (Confidence: 0.99)\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Government secretly plans to ban the Internet nationwide next month!\"\n",
    "label, confidence = classify_news(test_text)\n",
    "    \n",
    "print(f\"Prediction: {label} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dcd5447e-16db-41f9-83f6-58e89d4457b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_samples = [\n",
    "    \"NASA successfully lands rover on Mars, begins exploration mission.\",\n",
    "    \"World Health Organization reports decrease in global COVID-19 cases.\",\n",
    "    \"New climate policies aim to reduce carbon emissions by 40% by 2030.\",\n",
    "    \"Economists predict steady GDP growth for the upcoming quarter.\",\n",
    "    \"Scientists develop a new vaccine for malaria with 90% efficacy.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "afe6296d-4547-4652-b615-a2c141abc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_samples = [\n",
    "    \"Government secretly plans to ban the Internet nationwide next month!\",\n",
    "    \"Aliens have officially landed on Earth, but the media is hiding the truth!\",\n",
    "    \"Bill Gates implants microchips in vaccines to control the population.\",\n",
    "    \"New study proves the Earth is flat, NASA exposed for covering it up!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ce676706-ff0d-4c10-a6d7-7f04f53f517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: NASA successfully lands rover on Mars, begins exploration mission.\n",
      "Prediction: Real (Confidence: 0.93)\n",
      "\n",
      "Text: World Health Organization reports decrease in global COVID-19 cases.\n",
      "Prediction: Real (Confidence: 0.82)\n",
      "\n",
      "Text: New climate policies aim to reduce carbon emissions by 40% by 2030.\n",
      "Prediction: Real (Confidence: 0.96)\n",
      "\n",
      "Text: Economists predict steady GDP growth for the upcoming quarter.\n",
      "Prediction: Real (Confidence: 0.98)\n",
      "\n",
      "Text: Scientists develop a new vaccine for malaria with 90% efficacy.\n",
      "Prediction: Real (Confidence: 0.77)\n",
      "\n",
      "Text: Government secretly plans to ban the Internet nationwide next month!\n",
      "Prediction: Fake (Confidence: 0.99)\n",
      "\n",
      "Text: Aliens have officially landed on Earth, but the media is hiding the truth!\n",
      "Prediction: Fake (Confidence: 0.99)\n",
      "\n",
      "Text: Bill Gates implants microchips in vaccines to control the population.\n",
      "Prediction: Fake (Confidence: 0.89)\n",
      "\n",
      "Text: New study proves the Earth is flat, NASA exposed for covering it up!\n",
      "Prediction: Fake (Confidence: 0.99)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in real_news_samples + fake_news_samples:\n",
    "    label, confidence = classify_news(text)\n",
    "    print(f\"Text: {text}\\nPrediction: {label} (Confidence: {confidence:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "993ccb84-f6b9-4960-a8ca-2a24f89f7a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: NASA successfully lands rover on Mars, begins exploration mission.\n",
      "Prediction: Real (Confidence: 0.93)\n",
      "\n",
      "Text: Aliens have officially landed on Earth, but the media is hiding the truth!\n",
      "Prediction: Fake (Confidence: 0.99)\n",
      "\n",
      "Text: World Health Organization reports a decrease in global COVID-19 cases.\n",
      "Prediction: Real (Confidence: 0.97)\n",
      "\n",
      "Text: Chocolate cures cancer instantly, scientists confirm!\n",
      "Prediction: Real (Confidence: 0.97)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "# Load the trained DistilBERT model\n",
    "model_path = \"./text_classifier_model\"  # Update with the correct path\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Function to classify news using DistilBERT\n",
    "def classify_news_distilbert(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    prediction = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    label = \"Real\" if prediction == 0 else \"Fake\"\n",
    "    confidence = probabilities[0][prediction].item()\n",
    "    \n",
    "    return label, confidence\n",
    "\n",
    "# Test Cases\n",
    "test_texts = [\n",
    "    \"NASA successfully lands rover on Mars, begins exploration mission.\",\n",
    "    \"Aliens have officially landed on Earth, but the media is hiding the truth!\",\n",
    "    \"World Health Organization reports a decrease in global COVID-19 cases.\",\n",
    "    \"Chocolate cures cancer instantly, scientists confirm!\"\n",
    "]\n",
    "\n",
    "# Run predictions\n",
    "for text in test_texts:\n",
    "    label, confidence = classify_news_distilbert(text)\n",
    "    print(f\"Text: {text}\\nPrediction: {label} (Confidence: {confidence:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f3d55-57e2-498f-99d0-63913586897c",
   "metadata": {},
   "source": [
    "How is Confidence Measured in Your Fake News Classifier?\n",
    "In your text classification model (based on DistilBERT), confidence is calculated using the softmax function. This function converts the models raw output scores (logits) into probabilities between 0 and 1, representing the models confidence for each class (real or fake).\n",
    "\n",
    "1 Steps in Confidence Calculation\n",
    "1 Model Output (Logits):\n",
    "\n",
    "The model outputs two raw values (logits) before applying softmax.\n",
    "Example logits: [2.5, -1.2] (one value for each class).\n",
    "2 Apply Softmax to Get Probabilities:\n",
    "\n",
    "Convert logits into probabilities using the softmax function:\n",
    "P(real) = exp(2.5) / (exp(2.5) + exp(-1.2))\n",
    "P(fake) = exp(-1.2) / (exp(2.5) + exp(-1.2))\n",
    "Example probabilities: [0.92, 0.08]  92% confidence for real.\n",
    "3 Prediction & Confidence:\n",
    "\n",
    "The class with the highest probability is the final prediction.\n",
    "The probability of the predicted class is its confidence score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c363378-d755-4bd2-a26a-00b3f8c76e80",
   "metadata": {},
   "source": [
    "**Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe548e5-56fa-453e-b976-c051f01d5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be43397-c2bb-47f8-a187-086364253491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e9217e-d23d-4af3-b007-757b7eef7c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformation (Resizing + Normalization for ResNet)\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518a7880-6af0-4d1a-b79c-48d32cba9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet-50 (without classification head)\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Load pre-trained ResNet-50 using new syntax\n",
    "resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove last classification layer\n",
    "\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.text_encoder = distilbert\n",
    "        self.image_encoder = resnet\n",
    "        self.fc = nn.Linear(768 + 2048, 2)  # Combining BERT (768) + ResNet (2048)\n",
    "    \n",
    "    def forward(self, text_input, attention_mask, image_input):\n",
    "        # Text Feature Extraction\n",
    "        text_features = self.text_encoder(input_ids=text_input, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Image Feature Extraction\n",
    "        image_features = self.image_encoder(image_input).view(image_input.size(0), -1)\n",
    "        \n",
    "        # Concatenate text + image features\n",
    "        fused_features = torch.cat((text_features, image_features), dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fc(fused_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e54a3ecc-acf5-485d-8d3e-3bbc38cf1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = MultiModalClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04ff984a-1717-4545-ae3c-0293d64ce586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load and preprocess text + image\n",
    "def process_text(text):\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    return encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "def process_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image_transforms(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c01a70b-d108-40d9-b125-0ca70614d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Define your base folder\n",
    "BASE_FOLDER = r\"C:\\Users\\vedas\\Downloads\\Fake News Image Classifier.v7i.folder\"\n",
    "\n",
    "def process_text(text):\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    return encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "def process_image(image_path):\n",
    "    # Join base folder with image path\n",
    "    full_image_path = os.path.join(BASE_FOLDER, image_path)\n",
    "    image = Image.open(full_image_path).convert(\"RGB\")\n",
    "    return image_transforms(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4246c35-8c89-4922-b358-d0dd80421089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128]) torch.Size([1, 128]) torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "text_input, attention_mask = process_text(\"This is a fake news example.\")\n",
    "image_tensor = process_image(r\"train/fake/46345024_1200610680114496_8596215285885698048_n_jpg.rf.044f689955b99f933911aa646089a4bb.jpg\")\n",
    "\n",
    "print(text_input.shape, attention_mask.shape, image_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "300e9edb-4ed5-4831-8ce6-31a678c25f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Real\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "text = \"Breaking news: Major cyber attack exposes millions of records.\"\n",
    "image_path = \"train/fake/46345024_1200610680114496_8596215285885698048_n_jpg.rf.044f689955b99f933911aa646089a4bb.jpg\"  # Change this to your actual image path\n",
    "\n",
    "text_input, attention_mask = process_text(text)\n",
    "image_input = process_image(image_path)\n",
    "\n",
    "# Model prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(text_input, attention_mask, image_input)\n",
    "    prediction = torch.argmax(output, dim=1)\n",
    "    print(\"Predicted Class:\", \"Fake\" if prediction.item() == 0 else \"Real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f1e43fc-5677-49b2-b6f6-103fefe67b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Real\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "text = \"Government announces new policies to improve cybersecurity.\"\n",
    "image_path = \"train/real/290000748_10161560950821756_4329718605846500942_n_png.rf.862c603941276263a4e56e9e7c03dd3e.jpg\"  # Replace with actual image path\n",
    "\n",
    "# Preprocess input\n",
    "text_input, attention_mask = process_text(text)\n",
    "image_input = process_image(image_path)\n",
    "\n",
    "# Model prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(text_input, attention_mask, image_input)\n",
    "    prediction = torch.argmax(output, dim=1)\n",
    "    print(\"Predicted Class:\", \"Fake\" if prediction.item() == 0 else \"Real\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b602e6ec-16a7-4139-969b-566b1af82cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Base directory found: C:\\Users\\vedas\\Downloads\\Fake News Image Classifier.v7i.folder\\test\n",
      " Subdirectories: ['fake', 'real']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = r\"C:\\Users\\vedas\\Downloads\\Fake News Image Classifier.v7i.folder\\test\"  # Change to train if needed\n",
    "\n",
    "if os.path.exists(base_dir):\n",
    "    print(f\" Base directory found: {base_dir}\")\n",
    "    print(\" Subdirectories:\", os.listdir(base_dir))  # Check if 'real' and 'fake' exist\n",
    "else:\n",
    "    print(f\" Error: Base directory '{base_dir}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d4baf46-762c-42bb-b64f-061b103a71b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Checking 'real' directory...\n",
      "['290269670_10161560661591756_4441974433210634991_n_png.rf.25142f031e778a417a9ab21d1d126651.jpg', '317613700_542575967730317_3589927528493738163_n_jpg.rf.d44934c5c13643eaa93e7b1b46de67e8.jpg', '319058337_548062103848370_8552552818861718415_n_jpg.rf.8df908d16e610360f975552b77a0918e.jpg', '320819880_523402116396588_6428844743067984492_n_jpg.rf.357bfb587667f2d3bc60d2c88b4d80f8.jpg', '321416404_503976121714532_7985650480947430629_n_jpg.rf.a7c1920e2389d394456da30d17c03420.jpg']\n",
      " Checking 'fake' directory...\n",
      "['103957013_1698122423696650_5504362117726945206_n_jpg.rf.fa1d1e517fe64c4b5cac611b3b5a7dc7.jpg', '104669472_1698395713669321_5002246110583994663_n_jpg.rf.1a43c4fb90150ef4ae62ca61095eec96.jpg', '14424888_1083427928372551_4701235641640967098_o-260x146_png.rf.28651ab8eb085f8a565b851809b291cb.jpg', '189217620_2975724735972602_3920979210629420503_n_jpg.rf.f81b7bf741fba07dd398c864cf97d9bd.jpg', '192940211_2987031781508564_1938253883048499178_n_jpg.rf.3e88a8584efed53ea7d45a3cf07d340b.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = r\"C:\\Users\\vedas\\Downloads\\Fake News Image Classifier.v7i.folder\\test\"\n",
    "\n",
    "real_path = os.path.join(base_dir, \"real\")\n",
    "fake_path = os.path.join(base_dir, \"fake\")\n",
    "\n",
    "print(\" Checking 'real' directory...\")\n",
    "print(os.listdir(real_path)[:5])  # Show first 5 files (if any)\n",
    "\n",
    "print(\" Checking 'fake' directory...\")\n",
    "print(os.listdir(fake_path)[:5])  # Show first 5 files (if any))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd883d9e-4455-49d5-ad1c-226d77cc7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(base_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    categories = {\"real\": 1, \"fake\": 0}\n",
    "    \n",
    "    for category, label in categories.items():\n",
    "        folder_path = os.path.join(base_dir, category)\n",
    "\n",
    "        if not os.path.exists(folder_path):  \n",
    "            print(f\" Warning: Folder '{folder_path}' not found. Skipping...\")\n",
    "            continue  # Skip missing folders\n",
    "        \n",
    "        files = os.listdir(folder_path)\n",
    "        if not files:\n",
    "            print(f\" Warning: Folder '{folder_path}' is empty. Skipping...\")\n",
    "            continue  # Skip empty folders\n",
    "        \n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Process text and image (Modify as needed)\n",
    "            text_input, attention_mask = process_text(file_name)\n",
    "            image_input = process_image(file_path)\n",
    "\n",
    "            data.append((text_input, attention_mask, image_input))\n",
    "            labels.append(label)\n",
    "\n",
    "    if not data:\n",
    "        raise ValueError(\" No data found! Check dataset path or folder contents.\")\n",
    "\n",
    "    return data, torch.tensor(labels)\n",
    "\n",
    "# Load dataset\n",
    "X_test, y_test = load_dataset(base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64b83624-bdf6-4b6a-8e81-bd7618704292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import TensorDataset, DataLoader  #  Import TensorDataset\n",
    "\n",
    "# Define paths\n",
    "base_dir = r\"C:\\Users\\vedas\\Downloads\\Fake News Image Classifier.v7i.folder\\test\"\n",
    "categories = {\"real\": 1, \"fake\": 0}  # Assign labels\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(base_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for category, label in categories.items():\n",
    "        folder_path = os.path.join(base_dir, category)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Error: Folder '{folder_path}' not found.\")\n",
    "            continue  # Skip missing folders\n",
    "\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Process text and image\n",
    "            text_input, attention_mask = process_text(file_name)  # Modify if needed\n",
    "            image_input = process_image(file_path)\n",
    "\n",
    "            data.append((text_input, attention_mask, image_input))\n",
    "            labels.append(label)\n",
    "    \n",
    "    return data, torch.tensor(labels)\n",
    "\n",
    "# Load training/test dataset\n",
    "X_test, y_test = load_dataset(base_dir)  \n",
    "\n",
    "# Convert dataset into tensors\n",
    "text_inputs, attention_masks, image_inputs = zip(*X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "text_inputs = torch.stack(text_inputs)\n",
    "attention_masks = torch.stack(attention_masks)\n",
    "image_inputs = torch.stack(image_inputs)\n",
    "\n",
    "#  Now TensorDataset will work\n",
    "test_dataset = TensorDataset(text_inputs, attention_masks, image_inputs, y_test)\n",
    "\n",
    "# DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fdf6c383-ca9f-4095-85e8-19f9cad008f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_inputs shape: torch.Size([16, 1, 128])\n",
      "attention_masks shape: torch.Size([16, 1, 128])\n",
      "image_inputs shape: torch.Size([16, 1, 3, 224, 224])\n",
      "labels shape: torch.Size([16])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_inputs shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_inputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Expected: (batch_size, channels, height, width)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Expected: (batch_size,)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(text_inputs, attention_masks, image_inputs)\n\u001b[0;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mMultiModalClassifier.forward\u001b[1;34m(self, text_input, attention_mask, image_input)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_input, attention_mask, image_input):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Text Feature Extraction\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder(input_ids\u001b[38;5;241m=\u001b[39mtext_input, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Image Feature Extraction\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_encoder(image_input)\u001b[38;5;241m.\u001b[39mview(image_input\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:793\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    790\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(input_shape, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# (bs, seq_length)\u001b[39;00m\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m--> 793\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[0;32m    794\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    798\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m    799\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    803\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    804\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:442\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[1;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_4d_attention_mask_for_sdpa\u001b[39m(mask: torch\u001b[38;5;241m.\u001b[39mTensor, dtype: torch\u001b[38;5;241m.\u001b[39mdtype, tgt_len: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    430\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m    Creates a non-causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m    `(batch_size, key_value_length)`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m            The target length or query length the created mask shall have.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     _, key_value_length \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    443\u001b[0m     tgt_len \u001b[38;5;241m=\u001b[39m tgt_len \u001b[38;5;28;01mif\u001b[39;00m tgt_len \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m key_value_length\n\u001b[0;32m    445\u001b[0m     is_tracing \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mProxy) \u001b[38;5;129;01mor\u001b[39;00m is_torchdynamo_compiling()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for text_inputs, attention_masks, image_inputs, labels in test_loader:\n",
    "        print(f\"text_inputs shape: {text_inputs.shape}\")  # Expected: (batch_size, seq_length)\n",
    "        print(f\"attention_masks shape: {attention_masks.shape}\")  # Expected: (batch_size, seq_length)\n",
    "        print(f\"image_inputs shape: {image_inputs.shape}\")  # Expected: (batch_size, channels, height, width)\n",
    "        print(f\"labels shape: {labels.shape}\")  # Expected: (batch_size,)\n",
    "\n",
    "        outputs = model(text_inputs, attention_masks, image_inputs)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        break  # Stop after one batch for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43c99e5d-2297-4c7d-801a-724c89afc1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, text_input, attention_mask, image_input):\n",
    "    text_input = text_input.squeeze(1)  # Remove the extra dimension\n",
    "    attention_mask = attention_mask.squeeze(1)  # Remove the extra dimension\n",
    "    \n",
    "    # Text Feature Extraction\n",
    "    text_features = self.text_encoder(input_ids=text_input, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "    \n",
    "    # Image Feature Extraction\n",
    "    image_features = self.image_encoder(image_input).view(image_input.size(0), -1)\n",
    "    \n",
    "    # Fusion Layer\n",
    "    combined_features = torch.cat((text_features, image_features), dim=1)\n",
    "    output = self.classifier(combined_features)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d68cf2f5-93a8-4f61-9a6f-6215e1b8bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, text_input, attention_mask, image_input):\n",
    "    attention_mask = attention_mask.squeeze(1)  # Ensure correct shape\n",
    "    text_features = self.text_encoder(input_ids=text_input, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "    \n",
    "    image_features = self.image_encoder(image_input).view(image_input.size(0), -1)\n",
    "    \n",
    "    combined_features = torch.cat((text_features, image_features), dim=1)\n",
    "    output = self.classifier(combined_features)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7bd1b15e-46ed-4770-bca2-ba458446c617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Mask Shape: torch.Size([16, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Attention Mask Shape:\", attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fbfa8337-58f3-4cda-bc29-ed2e157e1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = attention_masks.squeeze(1)  # Remove unnecessary dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99e988c3-94c6-4333-8d16-36d822ea2f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Mask Shape: torch.Size([16, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Attention Mask Shape:\", attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2418ac02-c3f7-44d4-a1d4-282279c7c169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_inputs shape: torch.Size([16, 1, 128])\n",
      "attention_masks shape: torch.Size([16, 1, 128])\n",
      "image_inputs shape: torch.Size([16, 1, 3, 224, 224])\n",
      "labels shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(f\"text_inputs shape: {text_inputs.shape}\")\n",
    "print(f\"attention_masks shape: {attention_masks.shape}\")\n",
    "print(f\"image_inputs shape: {image_inputs.shape}\")\n",
    "print(f\"labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53de8c05-3aa1-41d7-8fad-f726daeb2e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(text_inputs.shape)  \n",
    "print(attention_masks.shape)  \n",
    "print(image_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77bc52c6-dac7-4442-8323-264450ade0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.38%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure your model is on the right device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text_inputs, attention_masks, image_inputs, labels in test_loader:\n",
    "        # Ensure correct dimensions\n",
    "        text_inputs = text_inputs.to(device).squeeze(1) if text_inputs.dim() == 3 else text_inputs.to(device)\n",
    "        attention_masks = attention_masks.to(device).squeeze(1) if attention_masks.dim() == 3 else attention_masks.to(device)\n",
    "        image_inputs = image_inputs.to(device).squeeze(1) if image_inputs.dim() == 5 else image_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text_inputs, attention_masks, image_inputs)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Update correct predictions count\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20c24326-0c42-45ac-9f30-d977d692f3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(labels.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "32328418-532e-4f83-ac12-38012a2dc715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Features: torch.Size([1, 768])\n",
      "Image Features: torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "text_features = model.text_encoder(text_inputs, attention_mask=attention_masks).last_hidden_state[:, 0, :]\n",
    "print(\"Text Features:\", text_features.shape)\n",
    "\n",
    "image_features = model.image_encoder(image_inputs).view(image_inputs.size(0), -1)\n",
    "print(\"Image Features:\", image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "31eece21-cfe8-4ed5-a5d5-400e35c227ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "be64be1c-c5af-47ec-aa2c-ca12e7b56f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c246db1e-979e-4ea2-af92-00282f848f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fake', 'real']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(DATASET_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7b910a1c-ed92-41ea-8a29-52d0517f794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, data_dir, tokenizer, max_length=128, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transform\n",
    "        self.data = []  # Store (text, image, label)\n",
    "\n",
    "        # Iterate over class folders (e.g., \"fake\", \"real\")\n",
    "        for label_folder in os.listdir(data_dir):\n",
    "            label_path = os.path.join(data_dir, label_folder)\n",
    "            if not os.path.isdir(label_path):\n",
    "                continue  # Skip if not a directory\n",
    "\n",
    "            label = 0 if label_folder.lower() == \"fake\" else 1  # Adjust based on folder names\n",
    "            \n",
    "            for filename in os.listdir(label_path):\n",
    "                if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check if it's an image\n",
    "                    image_path = os.path.join(label_path, filename)\n",
    "                    text_path = image_path.replace(\".jpg\", \".txt\").replace(\".png\", \".txt\")  # Assuming text has same name\n",
    "\n",
    "                    if os.path.exists(text_path):\n",
    "                        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            text_content = f.read().strip()\n",
    "                    else:\n",
    "                        text_content = \"\"  # Handle missing text case\n",
    "\n",
    "                    self.data.append((text_content, image_path, label))\n",
    "                    \n",
    "                    # Debugging output\n",
    "                    print(f\"Loaded: {filename} | Label: {label} | Text: {text_content[:50]}...\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, image_path, label = self.data[idx]\n",
    "\n",
    "        # Tokenize text\n",
    "        encoded_text = self.tokenizer(\n",
    "            text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return encoded_text[\"input_ids\"].squeeze(0), encoded_text[\"attention_mask\"].squeeze(0), image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "25f9acd6-d46b-4426-9f38-1aaf3eadf477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299780cb-67a9-4bf9-b92a-354188b744f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
